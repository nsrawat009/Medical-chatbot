{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NH2854\\medbot2\\medchat\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "# from llama_index import VectorStoreIndex,SimpleDirectoryReader\n",
    "# import docsearch\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.llms.ctransformers.CTransformers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"274c03e9-281b-46bc-a8c1-852d01185b0f\"\n",
    "PINECONE_API_ENV = \"gcp-starter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "data_path = os.path.join(current_directory, \"data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\NH2854\\\\medbot2\\\\data/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader) \n",
    "    documents = loader.load()\n",
    "    return documents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data/\")\n",
    "#extracted_data = load_pdf(r\"C:\\Users\\NH2854\\Medical-chatbot\\data\\Medical_book.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    # text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    # return tuple(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks = text_split(extracted_data)\n",
    "# print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result = embeddings.embed_query(\"Hello world\")\n",
    "# print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"c727e84c-88ca-4752-aea8-a70fbedf5980\")\n",
    "index = pc.Index(\"medical-chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the index name\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "# index=Pinecone.index('medical-chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.14375,\n",
       " 'namespaces': {'': {'vector_count': 14375}},\n",
       " 'total_vector_count': 14375}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# from uuid import uuid4\n",
    "\n",
    "# batch_limit = 100\n",
    "\n",
    "# texts = []\n",
    "# metadatas = []\n",
    "\n",
    "# # Assuming 'data' is the list of Document objects\n",
    "# for i, document in enumerate(tqdm(extracted_data)):\n",
    "#     # Extract metadata from the document\n",
    "#     metadata = {\n",
    "#         'source': document.metadata['source'],\n",
    "#         'page no': document.metadata['page']\n",
    "#     }\n",
    "#     # Extract text content from the document\n",
    "#     text = document.page_content  # Assuming 'page_content' contains the text content\n",
    "#     # Now we create chunks from the text content\n",
    "#     record_texts = text_splitter.split_text(text)\n",
    "#     # Create individual metadata dicts for each chunk\n",
    "#     record_metadatas = [{\n",
    "#         \"chunk\": j, \"text\": text, **metadata\n",
    "#     } for j, text in enumerate(record_texts)]\n",
    "#     # Append these to current batches\n",
    "#     texts.extend(record_texts)\n",
    "#     metadatas.extend(record_metadatas)\n",
    "#     # If we have reached the batch_limit we can add texts\n",
    "#     if len(texts) >= batch_limit:\n",
    "#         ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "#         embeds = embeddings.embed_documents(texts)\n",
    "#         index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "#         texts = []\n",
    "#         metadatas = []\n",
    "\n",
    "# # Process the remaining texts if any\n",
    "# if len(texts) > 0:\n",
    "#     ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "#     embeds = embeddings.embed_documents(texts)\n",
    "#     index.upsert(vectors=zip(ids, embeds, metadatas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [\n",
    "#     'this is the first chunk of text',\n",
    "#     'then another second chunk of text is here'\n",
    "# ]\n",
    "\n",
    "# res = embeddings.embed_documents(texts)\n",
    "# len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# index_name = 'medical-chatbot'\n",
    "# existing_indexes = [\n",
    "#     index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "# ]\n",
    "\n",
    "# # check if index already exists (it shouldn't if this is first time)\n",
    "# if index_name not in existing_indexes:\n",
    "#     # if does not exist, create index\n",
    "#     pc.create_index(\n",
    "#         index_name,\n",
    "#         dimension=384,  # dimensionality of ada 002\n",
    "#         metric='dotproduct',\n",
    "#         spec=spec\n",
    "#     )\n",
    "#     # wait for index to be initialized\n",
    "#     while not pc.describe_index(index_name).status['ready']:\n",
    "#         time.sleep(1)\n",
    "\n",
    "# # connect to index\n",
    "# index = pc.Index(index_name)\n",
    "# time.sleep(1)\n",
    "# # view index stats\n",
    "# index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.14375,\n",
       " 'namespaces': {'': {'vector_count': 14375}},\n",
       " 'total_vector_count': 14375}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"page_content\"  # the metadata field that contains our text\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "model\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is acne?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# create the query vector\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# xq = model.encode(query).tolist()\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m xq \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# now query\u001b[39;00m\n\u001b[0;32m      7\u001b[0m xc \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mquery(vector\u001b[38;5;241m=\u001b[39mxq, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, include_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"what is acne?\"\n",
    "\n",
    "# create the query vector\n",
    "xq = model.encode(query).tolist()\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=5, include_metadata=False)\n",
    "xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "\n",
    "import time\n",
    "\n",
    "def retrieve(query):\n",
    "    res =embeddings\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "\n",
    "    # get relevant contexts\n",
    "    contexts = []\n",
    "    time_waited = 0\n",
    "    while (len(contexts) < 3 and time_waited < 60 * 12):\n",
    "        res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
    "        contexts = contexts + [\n",
    "            x['metadata']['text'] for x in res['matches']\n",
    "        ]\n",
    "        print(f\"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...\")\n",
    "        time.sleep(15)\n",
    "        time_waited += 15\n",
    "\n",
    "    if time_waited >= 60 * 12:\n",
    "        print(\"Timed out waiting for contexts to be retrieved.\")\n",
    "        contexts = [\"No contexts retrieved. Try to answer the question yourself!\"]\n",
    "\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def complete(prompt):\n",
    "    # instructions\n",
    "    sys_prompt = \"You are a helpful assistant that always answers questions.\"\n",
    "    # query text-davinci-003\n",
    "    qa=RetrievalQA.from_chain_type(\n",
    "            llm=llm, \n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "             temperature=0\n",
    "\n",
    "               \n",
    "    )\n",
    "    return qa['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'HuggingFaceEmbeddings' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query_with_contexts \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m query_with_contexts\n",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m, in \u001b[0;36mretrieve\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      6\u001b[0m res \u001b[38;5;241m=\u001b[39membeddings\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# retrieve from Pinecone\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m xq \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# get relevant contexts\u001b[39;00m\n\u001b[0;32m     12\u001b[0m contexts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: 'HuggingFaceEmbeddings' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeApiAttributeError",
     "evalue": "ScoredVector has no attribute 'metadata' at ['['received_data', 'matches', 0]']['metadata']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPineconeApiAttributeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m xc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\NH2854\\medbot2\\medchat\\Lib\\site-packages\\pinecone\\core\\client\\model_utils.py:496\u001b[0m, in \u001b[0;36mModelNormal.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m--> 496\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiAttributeError(\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name),\n\u001b[0;32m    499\u001b[0m     [e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path_to_item, name] \u001b[38;5;28;01mif\u001b[39;00m e]\n\u001b[0;32m    500\u001b[0m )\n",
      "\u001b[1;31mPineconeApiAttributeError\u001b[0m: ScoredVector has no attribute 'metadata' at ['['received_data', 'matches', 0]']['metadata']"
     ]
    }
   ],
   "source": [
    "\n",
    "for result in xc['matches']:\n",
    "    print(f\"{round(result['score'], 5)}: {result['metadata']['text']}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32: Between two and four million Americans have AD;\n",
      "0.32: Between two and four million Americans have AD;\n",
      "0.31: Numerous epidemiological studies of populations are\n",
      "0.31: Numerous epidemiological studies of populations are\n",
      "0.27: and so there is a higherproportion of women in the most affected age groups.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"which metropolis has the highest number of people?\"\n",
    "\n",
    "# create the query vector\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "for result in xc['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint= \"model\"\n",
    "llm=CTransformers(model=\"model\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(prompt):\n",
    "    # instructions\n",
    "    sys_prompt = \"You are a helpful assistant that always answers questions.\"\n",
    "    # query text-davinci-003\n",
    "    qa=RetrievalQA.from_chain_type(\n",
    "            llm=llm, \n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "                  \n",
    "    )\n",
    "    return qa['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RetrievalQA' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 13\u001b[0m, in \u001b[0;36mcomplete\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# query text-davinci-003\u001b[39;00m\n\u001b[0;32m      5\u001b[0m qa\u001b[38;5;241m=\u001b[39mRetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m      6\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm, \n\u001b[0;32m      7\u001b[0m         chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m            \n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mqa\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'RetrievalQA' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "complete(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
